<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.427">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alexandre CAPEL">
<meta name="dcterms.date" content="2023-07-05">

<title>Rapport stage d’été 2023 : Crowdsourcing and Deep Learning with Peerannot</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="rapport_files/libs/clipboard/clipboard.min.js"></script>
<script src="rapport_files/libs/quarto-html/quarto.js"></script>
<script src="rapport_files/libs/quarto-html/popper.min.js"></script>
<script src="rapport_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="rapport_files/libs/quarto-html/anchor.min.js"></script>
<link href="rapport_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="rapport_files/libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="rapport_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="rapport_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="rapport_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

</head>

<body>

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Rapport stage d’été 2023 : Crowdsourcing and Deep Learning with Peerannot</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alexandre CAPEL </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 5, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  <div>
    <div class="abstract">
      <div class="abstract-title">Abstract</div>
      <p>&nbsp; &nbsp; Voici le rapport du stage effectué en Juin et Juillet 2023 à l’Institut Montpelliérain Alexander Grothendieck. Ce dernier va se découper en deux grosses parties :</p>
      <ul>
      <li>la première visant à rappeler et expliqué un article de crowdsourcing <span class="citation" data-cites="guan2018said">(<a href="#ref-guan2018said" role="doc-biblioref">Guan et al. 2018</a>)</span>.</li>
      <li>la seconde portera plus sur l’implémentation de la méthode de l’article à l’aide du package <a href="https://peerannot.github.io/">Peerannot</a>.</li>
      </ul>
    </div>
  </div>
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Sommaire</h2>
   
  <ul>
  <li><a href="#explication-de-larticle-who-said-what-guan2018said" id="toc-explication-de-larticle-who-said-what-guan2018said" class="nav-link active" data-scroll-target="#explication-de-larticle-who-said-what-guan2018said"><span class="header-section-number">1</span> Explication de l’article Who Said What <span class="citation" data-cites="guan2018said">(Guan et al. 2018)</span></a>
  <ul class="collapse">
  <li><a href="#mise-en-contexte" id="toc-mise-en-contexte" class="nav-link" data-scroll-target="#mise-en-contexte"><span class="header-section-number">1.1</span> Mise en contexte</a></li>
  <li><a href="#modèle-de-crowdsourcing" id="toc-modèle-de-crowdsourcing" class="nav-link" data-scroll-target="#modèle-de-crowdsourcing"><span class="header-section-number">1.2</span> Modèle de crowdsourcing</a></li>
  <li><a href="#exemples-de-statégie-pour-fusionner-les-opinions" id="toc-exemples-de-statégie-pour-fusionner-les-opinions" class="nav-link" data-scroll-target="#exemples-de-statégie-pour-fusionner-les-opinions"><span class="header-section-number">1.3</span> Exemples de statégie pour fusionner les opinions</a></li>
  <li><a href="#méthode-de-guan2018said" id="toc-méthode-de-guan2018said" class="nav-link" data-scroll-target="#méthode-de-guan2018said"><span class="header-section-number">1.4</span> Méthode de <span class="citation" data-cites="guan2018said">(Guan et al. 2018)</span></a></li>
  <li><a href="#pourquoi-les-erreurs-des-experts-ne-posent-pas-de-problème" id="toc-pourquoi-les-erreurs-des-experts-ne-posent-pas-de-problème" class="nav-link" data-scroll-target="#pourquoi-les-erreurs-des-experts-ne-posent-pas-de-problème"><span class="header-section-number">1.5</span> Pourquoi les erreurs des experts ne posent pas de problème</a></li>
  </ul></li>
  <li><a href="#implémentation-avec-peerannot" id="toc-implémentation-avec-peerannot" class="nav-link" data-scroll-target="#implémentation-avec-peerannot"><span class="header-section-number">2</span> Implémentation avec Peerannot</a>
  <ul class="collapse">
  <li><a href="#présentation-de-peerannot" id="toc-présentation-de-peerannot" class="nav-link" data-scroll-target="#présentation-de-peerannot"><span class="header-section-number">2.1</span> Présentation de Peerannot</a></li>
  <li><a href="#setup-expérimental" id="toc-setup-expérimental" class="nav-link" data-scroll-target="#setup-expérimental"><span class="header-section-number">2.2</span> Setup expérimental</a></li>
  <li><a href="#les-experts-sont-ils-fiables" id="toc-les-experts-sont-ils-fiables" class="nav-link" data-scroll-target="#les-experts-sont-ils-fiables"><span class="header-section-number">2.3</span> Les experts sont-ils fiables ?</a></li>
  <li><a href="#implémentation" id="toc-implémentation" class="nav-link" data-scroll-target="#implémentation"><span class="header-section-number">2.4</span> Implémentation</a></li>
  <li><a href="#comparaison-des-performances" id="toc-comparaison-des-performances" class="nav-link" data-scroll-target="#comparaison-des-performances"><span class="header-section-number">2.5</span> Comparaison des performances</a></li>
  </ul></li>
  <li><a href="#conclusion-et-perspectives" id="toc-conclusion-et-perspectives" class="nav-link" data-scroll-target="#conclusion-et-perspectives"><span class="header-section-number">3</span> Conclusion et perspectives</a></li>
  </ul>
</nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="explication-de-larticle-who-said-what-guan2018said" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="explication-de-larticle-who-said-what-guan2018said"><span class="header-section-number">1</span> Explication de l’article Who Said What <span class="citation" data-cites="guan2018said">(<a href="#ref-guan2018said" role="doc-biblioref">Guan et al. 2018</a>)</span></h2>
<section id="mise-en-contexte" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="mise-en-contexte"><span class="header-section-number">1.1</span> Mise en contexte</h3>
<p>&nbsp; &nbsp; Il arrive souvent qu’une donnée ne soit pas perçue de la même manière d’un travailleur à l’autre. Cela peut dépendre de plusieurs paramètres comme par exemple :</p>
<ul>
<li>l’expérience des travailleurs et leur fiabilité</li>
<li>la qualité des données : nombre de pixels dans une image par exemple.</li>
</ul>
<p>Dans le dataset CIFAR10H, où chaque image appartient à un ensemble de 10 classes disjointes, il peut arriver qu’il nous soit difficile de choisir entre deux classes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image_cifar10h.png" class="img-fluid figure-img" alt="A drawing of an elephant." width="200"></p>
<figcaption class="figure-caption">Chat ou chien ?</figcaption>
</figure>
</div>
<p>Un cas concret où la labélisation d’une image n’est pas évidente, c’est lorsque l’on doit effectuer un diagnositic médical. Dans <span class="citation" data-cites="guan2018said">(<a href="#ref-guan2018said" role="doc-biblioref">Guan et al. 2018</a>)</span>, on se place dans un cadre un groupe de médecins étiquettent des dépistages de la rétinopathie diabétique (diabetic retinopathy, DR) en cinq classes différentes :</p>
<ul>
<li>Pas de DR</li>
<li>DR bénin (non proliférative)</li>
<li>DR moyen</li>
<li>DR sévère</li>
<li>DR proliferative</li>
</ul>
<p>IMAGE DES DIFFERENTES CLASSES</p>
<p>Chaque image sera traité par une petite partie de ce groupe de médecin (aléatoirement) et chaque médecin va étudier une petite partie de l’ensemble des dépistages.</p>
<p>On peut facilement se persuader ici que chaque expert va donner son propre diagnostic (éventuellement différents les uns des autres) du fait du potentiel bruit qui se trouve dans ces images.</p>
<p>C’est dans ce cadre que <span class="citation" data-cites="guan2018said">(<a href="#ref-guan2018said" role="doc-biblioref">Guan et al. 2018</a>)</span> va chercher à trouver un “véritable” diagnostic, que l’on utilisera ensuite pour entraîner notre modèle.</p>
</section>
<section id="modèle-de-crowdsourcing" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="modèle-de-crowdsourcing"><span class="header-section-number">1.2</span> Modèle de crowdsourcing</h3>
<p>&nbsp; &nbsp; Pour répondre à la problématique, <span class="citation" data-cites="guan2018said">(<a href="#ref-guan2018said" role="doc-biblioref">Guan et al. 2018</a>)</span> ont d’abord fixé leur modèle. Dans le cadre des médecins, ils vont modéliser chacun d’eux avec la base d’Inception-v3 qu’ils vont ensuite entraîner avec les opinions du médecin qu’il modélise. Par exemple, si le dataset comporte un jeu de données où le total des médecins ayant étiqueté au moins une image est de 31, nous devrons entraîner 31 modèles.</p>
<div class="callout callout-style-simple callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Attention
</div>
</div>
<div class="callout-body-container callout-body">
<p>On voit assez rapidement, qu’en fonction du nombre d’expert du jeu de données, la mise en place du modèle sera plus au moins longue… De plus, on peut potentiellement être confronté à un nombre de donnée trop bas pour certains médecins. Ces contraintes sont à prendre en compte dans un tel modèle.</p>
</div>
</div>
</section>
<section id="exemples-de-statégie-pour-fusionner-les-opinions" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="exemples-de-statégie-pour-fusionner-les-opinions"><span class="header-section-number">1.3</span> Exemples de statégie pour fusionner les opinions</h3>
<p>&nbsp; &nbsp; Une fois que nous possèdons tout ces médecins modélisés, il va falloir réunir l’ensemble de leur opinion pour pouvoir en proposer une globale pour le modèle général et étiqueter de nouvelles données. Il y a des manières très intuitives que nous allons exposer ici.</p>
<p>On se place dans le cas où il y a <span class="math inline">N</span> experts et <span class="math inline">C</span> classes. On notera <span class="math inline">p_j(c)</span> la prédiction de la classe <span class="math inline">c</span> du modèle représentant l’expert <span class="math inline">j</span>.</p>
<section id="le-vote-majoritaire" class="level4" data-number="1.3.1">
<h4 data-number="1.3.1" class="anchored" data-anchor-id="le-vote-majoritaire"><span class="header-section-number">1.3.1</span> <strong>Le vote majoritaire</strong></h4>
<p>&nbsp; &nbsp; Ce cadre là est assez explicite. L’idée est d’organiser un vote des experts et sélectionner la classe ayant le plus de voix. Mathématiquement, si on note <span class="math inline">\hat{p}_j =\underset{c \in \llbracket 1,C \rrbracket}{\text{argmax}}~(p_j(c))</span>, alors la prédiction du modèle sera : <span class="math display">
\hat{Y}_c =  \frac{1}{N}\sum_{j=1}^{N} \mathbb{1}_{\hat{p}_j=c}
</span></p>
<p>Ici, la classe ayant la probabilité la plus haute sera celle ayant reçu le plus de voix.</p>
</section>
<section id="lexpert-moyen" class="level4" data-number="1.3.2">
<h4 data-number="1.3.2" class="anchored" data-anchor-id="lexpert-moyen"><span class="header-section-number">1.3.2</span> <strong>L’expert moyen</strong></h4>
<p>&nbsp; &nbsp; Ce paradigme est sûrement le plus simple et évident de la liste. Il consiste à utiliser les prédictions de chaque expert et de faire une moyenne en utilisant un poids uniforme pour chaque expert. On peut le calculer :</p>
<p><span class="math display">
Y_c = \frac{1}{N}\sum_{j=1}^{N}p_j(c)
</span></p>
<p>Il modélise ce qu’est censé dire un expert “moyen” (par rapport à l’ensemble des experts du modèle).</p>
</section>
<section id="pondérer-en-fonction-de-la-fiabilité" class="level4" data-number="1.3.3">
<h4 data-number="1.3.3" class="anchored" data-anchor-id="pondérer-en-fonction-de-la-fiabilité"><span class="header-section-number">1.3.3</span> <strong>Pondérer en fonction de la fiabilité</strong></h4>
<p>&nbsp; &nbsp; On peut assez rapidement constater que la stratégie précédente est assez rudimentaire. En effet, on pourrait lui reprocher de ne pas prendre en compte la fiabilité de l’expert : un médécin assez médiocre (si il y en a un) possèdera un poids égal à celui d’un expert excellent. On voudrait donc trouver un moyen de donner un poids plus important aux médecins plus fiable, on obtiendra alors une prédiction de la forme :</p>
<p><span class="math display">
Y_c = \sum_{j=1}^{N}w_jp_j(c) ~~~~~~~\text{où  } \sum_{j=1}^{N}w_j=1
</span></p>
<p>Un premier moyen de procédé est de se référer à un score de fiabilité qu’aurait chacun des experts et de baser les poids sur ces résultats là. Un score possible serait par exemple de regarder avec un dataset de test le nombre de bonne réponse que donne chacun des modèles. Si on note <span class="math inline">sc_j</span> le score du modèle <span class="math inline">j</span> par rapport au dataset de test, on posera :</p>
<p><span class="math display">
w_j = \frac{sc_j}{\sum_{k=1}^{N}sc_k}
</span></p>
</section>
</section>
<section id="méthode-de-guan2018said" class="level3" data-number="1.4">
<h3 data-number="1.4" class="anchored" data-anchor-id="méthode-de-guan2018said"><span class="header-section-number">1.4</span> Méthode de <span class="citation" data-cites="guan2018said">(<a href="#ref-guan2018said" role="doc-biblioref">Guan et al. 2018</a>)</span></h3>
<p>&nbsp; &nbsp; La méthode de <span class="citation" data-cites="guan2018said">(<a href="#ref-guan2018said" role="doc-biblioref">Guan et al. 2018</a>)</span> se base sur ce paradigme mais propose un calcul des poids différents. Ici, plutôt que de calculer séparément les poids <span class="math inline">w_j</span>, nous allons les entraîner comme un modèle classique, en utilisant un descente de gradient et une fonction de perte. La différence principale avec la méthode précédente est qu’elle permet de faire un lien entre les différents experts. Dans un tel cadre, même si un modèle possède les mêmes performances que les autres mais que ce dernier fait des erreurs très différentes, alors ça aura tendance à augmenter son poids car il sera plus utile au moment de faire la moyenne.</p>
<p><strong>Mais comment procéder ?</strong></p>
<p>Supposons que nous avons déja entainé nos <span class="math inline">N</span> modèles représentant chaque expert. Pour entrainer nos poids, nous allons réutiliser le même dataset avec lequel on a entrainé chacun de nos modèles. Ainsi, pour une image, nous allons utiliser les opinions des experts qui ont réellement étiqueté cette dernière pour produire une distribution associée. On définit cette distribution comme la prédiction target de cette image. Nous utiliserons ensuite la prédiction des modèles des autres experts pour optimiser les poids.</p>
<p>Par exemple, pour une image donnée, notons <span class="math inline">I</span> l’ensemble des médecins qui ont voté sur cette image. Notons également par <span class="math inline">y_i \in \mathbb{R^C}</span> l’opinion de l’expert <span class="math inline">i \in I</span>. Pour chaque expert <span class="math inline">j \in \llbracket 1,N \rrbracket</span>, on notera <span class="math inline">p_j \in \mathbb{R}^C</span> la prédiction de son modèle. On pose <span class="math inline">w_j</span> le poids du modèle <span class="math inline">j</span> (avec bien sûr <span class="math inline">\sum_j w_j =1</span>). On évaluera ensuite la fonction de perte en utilisant la prédiction suivante :</p>
<p><span class="math display">
\frac{\sum_{i \notin I} w_i p_i}{\sum_{j \notin I} w_j}
</span></p>
<p>et la cible sera :</p>
<p><span class="math display">
\frac{1}{|I|} \sum_{i \in I} y_i
</span></p>
<p>où nous mettrons à jour les paramètres par back propagation.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Remarque
</div>
</div>
<div class="callout-body-container callout-body">
<p>Nous n’avons à aucun moment ajouter de données supplémentaires, par rapport à celles utilisées pour entraîner les modèles d’experts.</p>
</div>
</div>
</section>
<section id="pourquoi-les-erreurs-des-experts-ne-posent-pas-de-problème" class="level3" data-number="1.5">
<h3 data-number="1.5" class="anchored" data-anchor-id="pourquoi-les-erreurs-des-experts-ne-posent-pas-de-problème"><span class="header-section-number">1.5</span> Pourquoi les erreurs des experts ne posent pas de problème</h3>
<p>Une question intuitive survient assez rapidement avec ce genre de modèle : les experts restant des humains, il peut arriver que ces derniers se trompent, ces erreurs ne nuisent-elles pas à la qualité de classification de nos modèles ?</p>
<p>Pour répondre à cette question, faisons quelques simulations. Nous allons prendre le célèbre dataset MNIST, sur lequel nous allons entrainer un modèle et ensuite estimer son taux d’erreur. Ce modèle sera vu comme le modèle d’un expert qui ne fait jamais d’erreur. Puis nous allons progressivement baisser la fiabilité de l’expert en changeant aléatoirement les labels du dataset d’entrainement avec une probabilité de plus en plus grande. Puis on estimera à nouveau chaque taux d’erreur.</p>
<p>Pour expérimenter, nous allons utiliser un modèle assez simple, dont le code est affiché ci-dessous :</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Voir le modèle</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> test_model(nn.Module):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">'''</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Réseau de neurone emprunter sur Pytorch.org.</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    '''</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(test_model, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">5</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">6</span>, <span class="dv">16</span>, <span class="dv">5</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">16</span><span class="op">*</span><span class="dv">4</span><span class="op">*</span><span class="dv">4</span>, <span class="dv">120</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">120</span>, <span class="dv">84</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">84</span>, <span class="dv">10</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv1(x)))</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv2(x)))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">16</span><span class="op">*</span><span class="dv">4</span><span class="op">*</span><span class="dv">4</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Nous allons donc utiliser les codes écrits en annexes pour pouvoir arriver à calculer notre courbe. Trève d’explication, voici le graphique que nous obtenons :</p>
<iframe src="./grap_noise.html" width="100%" height="500px">
</iframe>
<p>Regardons ce graphique un peu plus près. Nous voyons ici que lorsque notre expert est parfait et ne fait pas d’erreur, le modèle est assez performant, le taux d’erreur sur le jeu de données test est de <span class="math inline">1.14 \%</span>. Pourtant, on remarque ici qu’en corrompant les données avec une probabilité de <span class="math inline">0.5</span>, le taux d’erreur n’augmente seulement jusqu’à <span class="math inline">2.28 \%</span>, une différence qui n’est pas non plus aberrante. On peut même aller jusqu’à changer aléatoirement en label incorrect avec une probabilité de 0.8, le modèle qu’on entraînera n’aura qu’un taux d’erreur de <span class="math inline">7.43 \%</span> !</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Remarque
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>On voit que la suite n’est pas très intéressante vue que le taux d’erreur va croitre très rapidement vers <span class="math inline">1</span> après <span class="math inline">0.8</span>.</p>
</div>
</div>
</div>
<p>Ainsi, nous pouvons, à l’aide de cet exemple, constater que les erreurs des experts n’affectent pas drastiquement la qualité de prévison des modèles (même lorsque que l’expert n’est pas fiable est fait beaucoup d’erreur !). On peut espérer que cela se généralise dans d’autres modèles. (On a tester avec un autre dataset et une autre architecture, le résultat est bien moins satisfaisant)</p>
</section>
</section>
<section id="implémentation-avec-peerannot" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="implémentation-avec-peerannot"><span class="header-section-number">2</span> Implémentation avec Peerannot</h2>
<p>Il est temps de mettre en pratique ce que nous avons appris pour l’intégrer dans la librairie Peerannot.</p>
<section id="présentation-de-peerannot" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="présentation-de-peerannot"><span class="header-section-number">2.1</span> Présentation de Peerannot</h3>
<p>L’objectif principal de Peerannot et d’aider à la gestion de labels crowdsoucé dans des problèmes de classficiation. Il va permettre notamment de définir un format “standard” de dataset de crowdsourcing, pour pouvoir faciliter leur utilisation dès lors que nous voulons effectuer différentes stratégies d’apprentissage et ceux sans devoir recoder de nouveaux algorithmes pour chaque dataset.</p>
<div class="callout callout-style-simple callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Des informations supplémentaires
</div>
</div>
<div class="callout-body-container callout-body">
<p>Le formatage des datasets est expliqué sur le site de <a href="https://peerannot.github.io/">Peerannot</a>, et nous nous baserons sur ce format pour produire nos algorithmes.</p>
</div>
</div>
</section>
<section id="setup-expérimental" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="setup-expérimental"><span class="header-section-number">2.2</span> Setup expérimental</h3>
<section id="le-dataset" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="le-dataset"><span class="header-section-number">2.2.1</span> Le dataset</h4>
<p>Nous allons utiliser ici CIFAR10H, un dataset crowdsourcé d’images de taille <span class="math inline">32\times 32</span> colorées appartenant à 10 classes. On dispose de 9500 images d’entrainement, et 500 images de validation, chacunes annotées par un certains nombre de personne parmis 2571 experts. Chacune de ces 9500 images ont été étiquettées par au moins 63 personnes, et les experts ont voté pour au moins 181 images.</p>
<p>On disposera enfin d’un set de 50000 images test, pour évaluer les performances de nos futurs modèles.</p>
</section>
<section id="larchitecture" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="larchitecture"><span class="header-section-number">2.2.2</span> L’architecture</h4>
<p>Pour chaque modèle d’expert, nous utiliserons un ResNet18 préentrainé, sur lequel gélerons toutes les couches sauf la dernière (ou seulement la couche <code>model.fc</code>). On entraînera nos modèles avec une cross entropy loss et une descente de gradient stochastique.</p>
</section>
</section>
<section id="les-experts-sont-ils-fiables" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="les-experts-sont-ils-fiables"><span class="header-section-number">2.3</span> Les experts sont-ils fiables ?</h3>
</section>
<section id="implémentation" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="implémentation"><span class="header-section-number">2.4</span> Implémentation</h3>
<p>Comme nous l’avons expliqué plus tôt, l’implémentation va se séparer en deux parties distinctes : l’entraînement des modèles experts, et l’entraînement des poids.</p>
<section id="entraînement-des-modèles" class="level4" data-number="2.4.1">
<h4 data-number="2.4.1" class="anchored" data-anchor-id="entraînement-des-modèles"><span class="header-section-number">2.4.1</span> Entraînement des modèles</h4>
<p>Dans notre contexte, nous souhaitons entraîner 2571 modèles à partir d’un ResNet18.</p>
<div class="callout callout-style-simple callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Attention
</div>
</div>
<div class="callout-body-container callout-body">
<p>Nous savons que chaque modèle aura au moins un dataset de taille 181 mais au plus de taille 196. Il va donc falloir faire des concessions pendant l’entrainement</p>
</div>
</div>
<p>Pour se faire, nous allons alors avec des poids déjà préetrainé à l’aide de IMAGENET, geler les première couches de chaque modèle et n’entraîner que les dernières. La question est de savoir si nous utiliserons la dernière couche ou seulement la couche <code>model.fc</code>. Pour pouvoir trancher, nous avons donc commencé par tracer la courbe d’apprentissage et de validation sur les deux modèles, le code correspondant est <code>Code/crowd_models/learning_curve.py</code>:</p>
<iframe src="./learning_curve.html" width="100%" height="500px">
</iframe>
<p>De ce graphique, on déduit deux choses :</p>
<ul>
<li><p>il n’est pas nécessaire d’aller au delà de 100 epochs pour mieux apprendre</p></li>
<li><p><code>Net</code> (celui avec juste la couche <code>model.fc</code>) possède une performance quasiment équivalente à <code>Net2</code> qui demande plus de paramètres à estimer.(PAS CONVAINCU EN DISCUTER)</p></li>
</ul>
<p>Nous avons donc toutes nos informations pour pouvoir entraîner nos modèles correctement.</p>
<p>Or, nous remarquons que avec <code>peerannot</code>, le format du <code>.json</code> des labels n’est pas adapté à l’entraînement de nos experts. Nous allons donc devoir faire tout un travail de mise en place des datasets et les codes correspondant se trouverons dans le fichier <code>setup_datasets.py</code>. Ainsi, une fois le dataset prêt, nous avons entraîné chaque modèle tout un week-end et nous les avons enregistrés dans un dossier appelé <code>expert_models</code>. Voici le code pour l’entraînement (seulement):</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># dataset_all est une liste de dataset adapté pour </span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># l'entraînement de chaque modèle</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(i, end<span class="op">=</span><span class="st">"</span><span class="ch">\r</span><span class="st">"</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(dataset_all[i])<span class="op">%</span><span class="dv">32</span><span class="op">==</span><span class="dv">1</span>:</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        trainset <span class="op">=</span> DataLoader(dataset_all[i],  batch_size<span class="op">=</span><span class="dv">30</span>, </span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>                              shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        trainset <span class="op">=</span> DataLoader(dataset_all[i],  batch_size<span class="op">=</span><span class="dv">32</span>, </span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>                              shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    Net <span class="op">=</span> networks(<span class="st">'resnet18'</span>, n_classes<span class="op">=</span><span class="dv">10</span>, pretrained<span class="op">=</span><span class="va">True</span>).to(<span class="st">"cuda"</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    gel(Net)  <span class="co"># gèle selon la méthode que l'on a validé</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> optim.SGD(Net.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        train(trainset, Net, optimizer, loss, ongoing<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    torch.save(Net.state_dict(), <span class="ss">f"./expert_models/model-</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">_weights.pth"</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Tout le monde est entrainé !"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>On voit dans le code précédent un test avant le chargement du Dataset pour savoir si on entraîne les modèles avec des min-batches de taille 30 ou 32. En réalité, il se trouve que pour certains experts, le dernier mini-batch était de taille 1 ce qui entraîner une erreur pendant l’entraînement. Dès lors, voila ce qu’affiche le terminal :</p>
<pre class="{bash}"><code>ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 512, 1, 1])</code></pre>
<p>Cette erreur provient de (avec BN1 ça marche pas, problème avec le calcul de l’écart-type ?)</p>
</section>
<section id="entraînement-des-poids" class="level4" data-number="2.4.2">
<h4 data-number="2.4.2" class="anchored" data-anchor-id="entraînement-des-poids"><span class="header-section-number">2.4.2</span> Entraînement des poids</h4>
<p>Une fois que tous nos modèles sont bien enregistrer, il est temps de mettre en place l’entraînement de nos poids. Il a fallu d’abord créer un dataset comprenant l’image et la distribution associée à l’aide des votes des experts : nous avons accès facilement à ses information avec le format des dataset de <code>peerannot</code>. Toutes les fonctions utilisées pour construire le dataset est bien sur dans <code>setup_datasets.py</code>.</p>
<p>Un fois que nous avons le dataset, la première idée a été de charger tous les modèles dans une liste pour pouvoir ensuite appelé facilement ces derniers. Cependant, cela revenait à charger 2571 modèles et au bout du <span class="math inline">480^e</span> modèle, le terminal nous a affiché cette erreur :</p>
<pre class="{bash}"><code>outofmemoryerror: cuda out of memory. tried to allocate 20.00 mib (gpu 0; 22.03 gib total capacity; 21.26 gib already allocated; 18.88 mib free; 21.36 gib reserved in total by pytorch) if reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation. see documentation for memory management and pytorch_cuda_alloc_conf</code></pre>
<p>La mémoire nécessaire pour charger tous les modèles en même temps était trop importante pour le GPU, il a donc fallu changer de stratégie.</p>
<p>Ainsi, nous avons commencé par charger un modèle uniquement lorsqu’on en a besoin pour effectuer des calculs, seulement, cette méthode aussi était trop couteuse en mémoire.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Entraînement des poids</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> torch.ones(n_worker, dtype<span class="op">=</span><span class="bu">float</span>, requires_grad<span class="op">=</span><span class="va">True</span>)<span class="op">/</span>n_worker</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>param <span class="op">=</span> nn.Parameter(w, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD([param], lr<span class="op">=</span><span class="fl">0.01</span>,momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch, (X, lab) <span class="kw">in</span> <span class="bu">enumerate</span>(trainset):</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> networks(<span class="st">'resnet18'</span>, n_classes<span class="op">=</span><span class="dv">10</span>, pretrained<span class="op">=</span><span class="va">False</span>).to(<span class="st">"cuda"</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> torch.zeros((batch_size, n_classe), requires_grad<span class="op">=</span><span class="va">False</span>).to(<span class="st">"cuda"</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        work_t <span class="op">=</span> get_workers(X[i], dataset, obj_train, tot_worker)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        tot <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> work_t:</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>            p[i] <span class="op">=</span> p[i] <span class="op">+</span> worker_pred(model, X, <span class="bu">int</span>(k))[i]<span class="op">*</span>param[<span class="bu">int</span>(k)]    <span class="co">#prédiction du worker k sur l'item i</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>            tot <span class="op">=</span> tot <span class="op">+</span> param[<span class="bu">int</span>(k)]</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        p[i] <span class="op">=</span> p[i]<span class="op">/</span>tot</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    lossRes <span class="op">=</span> loss(p, lab.to(<span class="st">"cuda"</span>))</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    lossRes.backward()</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>(les paramètres sont chargés dans la fonction <code>worker_pred</code> situé dans le module <code>models.py</code>).</p>
<p>Mais à ce moment là, une autre erreur a fait surface :</p>
<pre class="{bash}"><code>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [10]], which is output 0 of AsStridedBackward0, is at version 1666; expected version 1665 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).</code></pre>
<p>Il nous étais impossible de calculer le gradient (et donc fait la backpropagation…). On a du modifier la structure du code pour pouvoir nous permettre ce calcul primordial. Puis une nouvelle fois un problème de mémoire sur le GPU est arrivé, on a donc du rajouter trois lignes de code à la fin du calcul des prédictions pour supprimer le modèle et nettoyer la mémoire du GPU pour retirer toute trace éventuelle. Nous obtenons le code suivant :</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>tot_worker <span class="op">=</span> <span class="bu">list</span>(t.keys())</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> torch.ones(n_worker, dtype<span class="op">=</span>torch.<span class="bu">float</span>, requires_grad<span class="op">=</span><span class="va">True</span>) <span class="op">/</span> n_worker</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>param <span class="op">=</span> nn.Parameter(w)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD([param], lr<span class="op">=</span><span class="fl">0.01</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch, (X, lab) <span class="kw">in</span> <span class="bu">enumerate</span>(trainset):</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> torch.zeros((batch_size, n_classe)).to(<span class="st">"cuda"</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> networks(<span class="st">'resnet18'</span>, n_classes<span class="op">=</span><span class="dv">10</span>, pretrained<span class="op">=</span><span class="va">False</span>).to(<span class="st">"cuda"</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        work_t <span class="op">=</span> get_workers(X[i], dataset, obj_train, tot_worker)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        p <span class="op">=</span> torch.zeros((n_worker, n_classe))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        sel_w <span class="op">=</span> torch.zeros((n_worker, <span class="dv">1</span>))</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> work_t:</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>            p[<span class="bu">int</span>(k)] <span class="op">=</span> worker_pred(model, X, <span class="bu">int</span>(k))[i].detach()</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>            sel_w[<span class="bu">int</span>(k)] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        pred[i] <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>sel_w.t().matmul(param))<span class="op">*</span>p.t().matmul(param)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">del</span> model</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    gc.collect()</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    torch.cuda.empty_cache()</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    lossRes <span class="op">=</span> loss(pred, lab.to(<span class="st">"cuda"</span>))</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    lossRes.backward()</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># l'article ne précise pas si il faut softmax les poids </span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># pendant la boucle ou à la fin.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finalement, le problème le plus embêtant est celui que l’on obtient avec ce code : le temps de calcul. Il semblerait que dans cette configuration, l’entraînement des poids pour seulement une epoch va durer 11 jours ! Le stage sera terminer avant la fin de la deuxième epoch. J’ai donc tenter de contourné le problème en ne solicitant qu’un sous ensemble (déterminé aléatoirement pour chaque mini-batch) des modèles pour réduire le temps de calcul. Le code final est donc celui-ci :</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Voir le modèle</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>n_worker <span class="op">=</span> <span class="bu">len</span>(t)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> torch.ones(n_worker, dtype<span class="op">=</span>torch.<span class="bu">float</span>, requires_grad<span class="op">=</span><span class="va">True</span>) <span class="op">/</span> n_worker</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>param <span class="op">=</span> nn.Parameter(w)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD([param], lr<span class="op">=</span><span class="fl">0.01</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>err <span class="op">=</span> np.zeros(<span class="bu">len</span>(trainset))</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch, (X, lab) <span class="kw">in</span> <span class="bu">enumerate</span>(trainset):</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    workers <span class="op">=</span> sample(<span class="bu">list</span>(t.keys()), <span class="dv">700</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> networks(<span class="st">'resnet18'</span>, n_classes<span class="op">=</span><span class="dv">10</span>, pretrained<span class="op">=</span><span class="va">False</span>).to(<span class="st">"cuda"</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> batch<span class="op">&lt;</span><span class="bu">len</span>(trainset)<span class="op">-</span><span class="dv">1</span>:</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># pour faire coïncider la taille des batch pour savoir si il s'agit du dernier</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> torch.zeros((batch_size, n_classe)).to(<span class="st">"cuda"</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>            work_t <span class="op">=</span> get_workers(X[i], dataset, obj_train, workers)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>            p <span class="op">=</span> torch.zeros((n_worker, n_classe))</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>            sel_w <span class="op">=</span> torch.zeros((n_worker, <span class="dv">1</span>))</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> k <span class="kw">in</span> work_t:</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>                p[<span class="bu">int</span>(k)] <span class="op">=</span> worker_pred(model, X, <span class="bu">int</span>(k))[i].detach()</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>                sel_w[<span class="bu">int</span>(k)] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>            pred[i] <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>sel_w.t().matmul(param))<span class="op">*</span>p.t().matmul(param)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        new_bsize <span class="op">=</span> <span class="bu">len</span>(obj_train)<span class="op">%</span>batch_size</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> torch.zeros((new_bsize, n_classe)).to(<span class="st">"cuda"</span>)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(new_bsize):</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>            work_t <span class="op">=</span> get_workers(X[i], dataset, obj_train, workers)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>            p <span class="op">=</span> torch.zeros((n_worker, n_classe))</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>            sel_w <span class="op">=</span> torch.zeros((n_worker, <span class="dv">1</span>))</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> k <span class="kw">in</span> work_t:</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>                p[<span class="bu">int</span>(k)] <span class="op">=</span> worker_pred(model, X, <span class="bu">int</span>(k))[i].detach()</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>                sel_w[<span class="bu">int</span>(k)] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>            pred[i] <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>sel_w.t().matmul(param))<span class="op">*</span>p.t().matmul(param)</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">del</span> model</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>    gc.collect()</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    torch.cuda.empty_cache()</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>    lossRes <span class="op">=</span> loss(pred, lab.to(<span class="st">"cuda"</span>))</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    err[batch] <span class="op">=</span> lossRes.item()</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    lossRes.backward()</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Pour faciliter la lisibilité du code et l’accès à certaines information, nous avons créer la classe <code>CIFAR10H</code> disponible dans le fichier <code>models.py</code>.</p>
</section>
</section>
<section id="comparaison-des-performances" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="comparaison-des-performances"><span class="header-section-number">2.5</span> Comparaison des performances</h3>
<p>Après avoir entraîner nos poids, nous allons pouvoir comparer la méthode d’agrégation avec d’autres stratégies. Le package <code>peerannot</code> a déjà plusieurs stratégies disponibles notamment la plus rudimentaire : le vote majoritaire.</p>
</section>
</section>
<section id="conclusion-et-perspectives" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="conclusion-et-perspectives"><span class="header-section-number">3</span> Conclusion et perspectives</h2>
<p>Notre modèle est donc ?</p>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-guan2018said" class="csl-entry" role="listitem">
Guan, Melody Y., Varun Gulshan, Andrew M. Dai, and Geoffrey E. Hinton. 2018. <span>“Who Said What: Modeling Individual Labelers Improves Classification.”</span> <a href="https://arxiv.org/abs/1703.08774">https://arxiv.org/abs/1703.08774</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>